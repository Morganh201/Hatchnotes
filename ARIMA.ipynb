{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4caebc7f",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03413d",
   "metadata": {},
   "source": [
    "Now that we have analyzed the time series, we now must be able to forecast or predict future values based on previous values of the time series. One such model we can use is the ARIMA model, or the AutoRegressive Integrated Moving Average, a univariate time series forecasting model that only considers time as the independent variable. An ARIMA model can be characterized by 3 terms, $p, d, q$, where $p$ is the order of the autoregressive term, $q$ is the order of the moving average terms, and $d$ is the degree of differencing required to make the time series stationary. If the time series has seasonal patterns, then we must add seasonal terms to the model, and thus we can SARIMA, or Seasonal ARIMA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6ef1b",
   "metadata": {},
   "source": [
    "The autoregressive term, or $p$, refers to the number of lags of the target variable $y$ to be used as predictors, and the moving average term, or $q$, refers to the number of lagged forecastng errors of the autoregressive model that go into the ARIMA model. It follows that the ARIMA model is a combination of two other models, the autoregressive model(AR) and the Moving Average model(MA). The AR model is defined with the following formula: $$ Y_t = \\mu + \\beta_1 Y_{t-1} + \\beta_2Y_{t-2}+ \\dots + \\beta_pY_{t-p} + \\epsilon_t$$ where $\\mu$ is the mean of the series, $Y_t$ is a function in terms of the lags of up to lag $p$, $\\beta_i$ refers to the coefficient of $Y_{t-i}$, which the model estimates, or learns, and $\\epsilon_t$ refers to the error term, the difference between the actual value $Y_t$ and the predicted value $\\hat{Y_t}, \\epsilon_t = Y_t - \\hat{Y_t}$. The MA model is defined similarily: $$Y_{t} = \\mu + \\epsilon_t+ \\phi_1\\epsilon_{t-1}+\\dots+\\phi_q\\epsilon_{t-q}$$ where the error terms $\\epsilon_{t-i}$ refers to the error terms of the autoregressive model respective to the lagged term $Y_{t-i}$, and $\\phi_i$ is the coefficients the model is predicting. The error term $\\epsilon_t$ is obtained from the following autoregressive model: $$ Y_t = \\mu + \\beta_1Y_{t-1} + \\dots + \\beta_tY_0 + \\epsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54d035",
   "metadata": {},
   "source": [
    "All together, the ARIMA model is a function $$Y_t = \\mu + \\beta_1Y_{t-1} + \\dots + \\beta_pY_{t-p} + \\epsilon_t + \\phi_1\\epsilon_{t-1} + \\dots + \\phi_q\\epsilon_{t-q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0f368",
   "metadata": {},
   "source": [
    "Now, it is our job to find the values $p, q$, and $d$ so that our ARIMA model is efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d450f",
   "metadata": {},
   "source": [
    "### Finding order of differencing $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ec8a9",
   "metadata": {},
   "source": [
    "To find the degree of differencing $d$, it follows that we must interpret the autocorrelation function for each degree of differencing. If we see that the autocorrelations are positive and statistically significant for many number of lags, then the series need more degrees of differencing, as previous lags correlate significantly with the current value. On the other hand, if the autocorrelation for lag 1 is negative, then the series might be over-differenced, and a model with a lesser degree of differencing will probably work better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb923",
   "metadata": {},
   "source": [
    "If we want to quantify the results on finding $d$, then we can use the ADFuller test and the KPSS test and analyze their p values. Thus, we can apply $d+1$ differencing to the time series and check the statistical tests until the tests agree that the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the time series and its autocorrelation, then use ADFuller and KPSS to analyse the stationarity of the time series\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, axes = plt.subplots(3, 2,figsize=(12, 12))\n",
    "axes[0, 0].plot(df['value'])\n",
    "axes[1, 0].plot(df['value'].diff())\n",
    "axes[2, 0].plot(df['value'].diff().diff())\n",
    "plot_acf(df['value'].dropna(), ax=axes[0, 1])\n",
    "plot_acf(df['value'].diff().dropna(), ax=axes[1, 1])\n",
    "plot_acf(df['value'].diff().diff().dropna(), ax=axes[2, 1])\n",
    "plt.show()\n",
    "\n",
    "# d=0\n",
    "result = adfuller(df['value'])\n",
    "print(f'ADF Statistic of d=0: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "result = kpss(df['value'])\n",
    "print(f'KPSS Statistic of d=0: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# d=1\n",
    "result = adfuller(df['value'].diff().dropna())\n",
    "print(f'ADF Statistic of d=1: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "result = kpss(df['value'].diff().dropna())\n",
    "print(f'KPSS Statistic of d=1: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# d=2\n",
    "result = adfuller(df['value'].diff().diff().dropna())\n",
    "print(f'ADF Statistic of d=2: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "result = kpss(df['value'].diff().diff().dropna())\n",
    "print(f'KPSS Statistic of d=2: {result[0]}')\n",
    "print(f'p-value: {result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c10e9",
   "metadata": {},
   "source": [
    "### Finding order of AR term $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e8f68",
   "metadata": {},
   "source": [
    "To find the AR term of the ARIMA model, it follows that we will inspect the partial autocorrelation function(PACF). The PACF is different from the ACF in the sense that it only considers the direct correlation between the current value and its lagged value, disregarding the intermediate correlations existant between both points. In the AR equation, it follows that the partial autocorrelation between a value and its lagged value is represented by the coefficient of that lag. For example, if we have the model equation of AR(3) as: $$Y_t = \\mu + \\beta_1 Y_{t-1} + \\beta_2Y_{t-2} + \\beta_3Y_{t-3} + \\epsilon_t$$ then it follows that the partial autocorrelation of $Y_{t-3}$ is $\\beta_3$. The AR equation uses the PACF because mathematically it is better to only consider the direct correlations between points and their lagged version of themselves, rather than factoring in the many indirect correlations in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d611b1",
   "metadata": {},
   "source": [
    "Now, to find the $p$ value, the number of $Y_i$ terms we include in the equation, we must look at the PACF graph with respect to the degree of differencing we have chosen in the previous step, $d$. Then, we count the number of consecutive statistically significant points after lag 0; choosing points inside the significant interval is unnecessary and might cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7db593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Plot time series with d=1 and pacf with d=1 of time series\n",
    "plt.rcParams.update({'figure.figsize':(12, 6), 'figure.dpi':120})\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(df['flow'].diff().dropna())\n",
    "plot_pacf(df['flow'].diff().dropna(), ax=axes[1], lags=24*365)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a812fd4",
   "metadata": {},
   "source": [
    "### Finding order of MA term $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceaa893",
   "metadata": {},
   "source": [
    "To find the MA term of the ARIMA model, it follows that we will look at the autocorrelation function based on the degree of differencing $d$, and find the number of consecutive statistically significant points after lag 0, which corresponds to the value $q$. Unlike the autoregressive model, it follows that the moving average model factors in the previous error terms of the time series and thus calculates the next value based on those error terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb249b4",
   "metadata": {},
   "source": [
    "### Building ARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159443e1",
   "metadata": {},
   "source": [
    "Like all machine learning models, to test a model, we must be able to split the data into training data and validation data. However, unlike machine learning models, it follows that the order must stay intact, as spliting the data by taking random points will make the ARIMA model ineffective. Thus, using normal cross-validation techniques in standard machine learning algorithms, such as k-fold cross-validation, is not possible in time series data, as we cannot use future data to predict past data. Therefore, we will split the time series based on time; we split the data into past data as our training data and future data as our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7aec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit()\n",
    "TimeSeriesSplit(max_train_size=None, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51463bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "model = ARIMA(df['value'], order=(2, 1, 2))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0ea87",
   "metadata": {},
   "source": [
    "From the summary it follows that we can get the coefficients as well as their p values. We can use those p values to make some readjustments to the parameters of our ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3ee86",
   "metadata": {},
   "source": [
    "We can also automate the ARIMA process in order to get the results quicker by using the pmdarima python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea80b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "auto_arima = pm.auto_arima(X_train)\n",
    "auto_pred = auto_arima.predict(n_periods=len(val_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e82df",
   "metadata": {},
   "source": [
    "### Accuracy Metrics for ARIMA forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14689c5",
   "metadata": {},
   "source": [
    "When evaluating the performance of a time series model such as ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bc64c",
   "metadata": {},
   "source": [
    "To compute the accuracy of our ARIMA model, we can use the mean absolute percentage error, correlation between the actual and the forecast values, and min-max error, so that our error values can be comparable to other data with different scales. If we only need to compare the error values with different models on the same dataset, then we can simply use the mean absolute error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
